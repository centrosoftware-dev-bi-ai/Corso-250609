{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Introduzione al Web Scraping con Python\n",
    "\n",
    "In questo notebook mostreremo come effettuare Web Scraping usando due librerie leggere e potenti: `requests` e `BeautifulSoup`.\n",
    "\n",
    "Eviteremo strumenti complessi come `Selenium`, così che il codice sia eseguibile anche su ambienti come [Binder](https://mybinder.org).\n",
    "\n",
    "Obiettivo: Estrarre titoli e link delle notizie dalla homepage di un sito web semplice e statico."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Cos'è il Web Scraping?\n",
    "\n",
    "Il Web Scraping è la tecnica di estrazione automatica di contenuti da pagine web.\n",
    "\n",
    "### Componenti principali:\n",
    "- **HTTP client**: per inviare richieste e ricevere le pagine HTML (es. `requests`)\n",
    "- **Parser HTML**: per analizzare e navigare la struttura del documento HTML (es. `BeautifulSoup`)\n",
    "\n",
    "**Attenzione**: Prima di effettuare scraping su un sito web, verifica i suoi termini di servizio e il file `robots.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. Esempio base: Estrazione titoli da un sito web\n",
    "\n",
    "Come esempio useremo il sito [https://quotes.toscrape.com/](https://quotes.toscrape.com/), pensato proprio per fare pratica con lo scraping.\n",
    "\n",
    "Vogliamo ottenere una lista delle citazioni presenti nella homepage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://quotes.toscrape.com/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Analisi della risposta\n",
    "\n",
    "Se il codice di stato è `200`, la richiesta ha avuto successo e possiamo analizzare il contenuto HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = response.text\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 3. Navigare il DOM con BeautifulSoup\n",
    "\n",
    "Analizziamo la struttura del documento per trovare dove sono contenute le citazioni. Ogni citazione è in un elemento `<div class=\"quote\">`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes = soup.find_all(\"div\", class_=\"quote\")\n",
    "len(quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 4. Estrazione dei dati\n",
    "\n",
    "Per ogni citazione, estraiamo:\n",
    "- Il testo della citazione\n",
    "- L'autore\n",
    "- Le tag associate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "for quote in quotes[:3]:\n",
    "    text = quote.find(\"span\", class_=\"text\").get_text()\n",
    "    author = quote.find(\"small\", class_=\"author\").get_text()\n",
    "    tags = [tag.get_text() for tag in quote.find_all(\"a\", class_=\"tag\")]\n",
    "    print(f\"CITAZIONE: {text}\")\n",
    "    print(f\"  AUTORE: {author}\")\n",
    "    print(f\"  TAG: {tags}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 5. Scraping di più pagine\n",
    "\n",
    "Il sito ha una paginazione classica con un link `Next`.\n",
    "\n",
    "Vediamo come fare scraping su più pagine navigando automaticamente finché esiste la pagina successiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_data = []\n",
    "url = \"https://quotes.toscrape.com/\"\n",
    "\n",
    "while url:\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    for quote in soup.find_all(\"div\", class_=\"quote\"):\n",
    "        text = quote.find(\"span\", class_=\"text\").get_text()\n",
    "        author = quote.find(\"small\", class_=\"author\").get_text()\n",
    "        tags = [tag.get_text() for tag in quote.find_all(\"a\", class_=\"tag\")]\n",
    "        quotes_data.append({\n",
    "            \"text\": text,\n",
    "            \"author\": author,\n",
    "            \"tags\": tags\n",
    "        })\n",
    "\n",
    "    next_btn = soup.find(\"li\", class_=\"next\")\n",
    "    if next_btn:\n",
    "        next_link = next_btn.find(\"a\")[\"href\"]\n",
    "        url = \"https://quotes.toscrape.com\" + next_link\n",
    "    else:\n",
    "        url = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(quotes_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Salvataggio dei dati in CSV\n",
    "\n",
    "Salviamo la lista di citazioni estratte in un file CSV, per poterla usare o analizzare successivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_filename = \"quotes.csv\"\n",
    "with open(csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"text\", \"author\", \"tags\"])\n",
    "    writer.writeheader()\n",
    "    for row in quotes_data:\n",
    "        row[\"tags\"] = \", \".join(row[\"tags\"])\n",
    "        writer.writerow(row)\n",
    "print(f\"Dati salvati su {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Esempio di autenticazione Basic HTTP\n",
    "\n",
    "Se un sito è protetto da autenticazione Basic HTTP, puoi passare le credenziali con `requests`.\n",
    "\n",
    "Per esempio, se un sito richiede username e password:\n",
    "\n",
    "- user: `admin`\n",
    "- password: `password123`\n",
    "\n",
    "Puoi fare così:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_url = \"https://httpbin.org/basic-auth/admin/password123\"\n",
    "response_auth = requests.get(auth_url, auth=(\"admin\", \"password123\"))\n",
    "\n",
    "if response_auth.status_code == 200:\n",
    "    print(\"Autenticazione riuscita\")\n",
    "    data = response_auth.json()\n",
    "    print(data)\n",
    "else:\n",
    "    print(f\"Autenticazione fallita con codice {response_auth.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## 6. Conclusioni\n",
    "\n",
    "In questo notebook abbiamo imparato a:\n",
    "- Effettuare richieste HTTP con `requests`\n",
    "- Analizzare e navigare un documento HTML con `BeautifulSoup`\n",
    "- Estrarre dati strutturati da più pagine\n",
    "\n",
    "### Prossimi passi:\n",
    "- Salvare i dati in CSV o JSON\n",
    "- Gestire siti più complessi con contenuti dinamici (JavaScript)\n",
    "- Analizzare il `robots.txt` per rispetto delle regole di scraping\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
