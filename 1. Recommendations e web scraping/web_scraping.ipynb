{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Introduzione al Web Scraping con Python\n",
    "\n",
    "In questo notebook mostreremo come effettuare Web Scraping usando due librerie leggere e potenti: `requests` e `BeautifulSoup`.\n",
    "\n",
    "Eviteremo strumenti complessi come `Selenium`, così che il codice sia eseguibile anche su ambienti come [Binder](https://mybinder.org).\n",
    "\n",
    "Obiettivo: Estrarre titoli e link delle notizie dalla homepage di un sito web semplice e statico."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Cos'è il Web Scraping?\n",
    "\n",
    "Il Web Scraping è la tecnica di estrazione automatica di contenuti da pagine web.\n",
    "\n",
    "### Componenti principali:\n",
    "- **HTTP client**: per inviare richieste e ricevere le pagine HTML (es. `requests`)\n",
    "- **Parser HTML**: per analizzare e navigare la struttura del documento HTML (es. `BeautifulSoup`)\n",
    "\n",
    "**Attenzione**: Prima di effettuare scraping su un sito web, verifica i suoi termini di servizio e il file `robots.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. Esempio base: Estrazione titoli da un sito web\n",
    "\n",
    "Come esempio useremo il sito [https://quotes.toscrape.com/](https://quotes.toscrape.com/), pensato proprio per fare pratica con lo scraping.\n",
    "\n",
    "Vogliamo ottenere una lista delle citazioni presenti nella homepage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://quotes.toscrape.com/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Analisi della risposta\n",
    "\n",
    "Se il codice di stato è `200`, la richiesta ha avuto successo e possiamo analizzare il contenuto HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = response.text\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 3. Navigare il DOM con BeautifulSoup\n",
    "\n",
    "Analizziamo la struttura del documento per trovare dove sono contenute le citazioni. Ogni citazione è in un elemento `<div class=\"quote\">`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes = soup.find_all(\"div\", class_=\"quote\")\n",
    "len(quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 4. Estrazione dei dati\n",
    "\n",
    "Per ogni citazione, estraiamo:\n",
    "- Il testo della citazione\n",
    "- L'autore\n",
    "- Le tag associate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "for quote in quotes[:3]:\n",
    "    text = quote.find(\"span\", class_=\"text\").get_text()\n",
    "    author = quote.find(\"small\", class_=\"author\").get_text()\n",
    "    tags = [tag.get_text() for tag in quote.find_all(\"a\", class_=\"tag\")]\n",
    "    print(f\"CITAZIONE: {text}\")\n",
    "    print(f\"  AUTORE: {author}\")\n",
    "    print(f\"  TAG: {tags}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 5. Scraping di più pagine\n",
    "\n",
    "Il sito ha una paginazione classica con un link `Next`.\n",
    "\n",
    "Vediamo come fare scraping su più pagine navigando automaticamente finché esiste la pagina successiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_data = []\n",
    "url = \"https://quotes.toscrape.com/\"\n",
    "\n",
    "while url:\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    for quote in soup.find_all(\"div\", class_=\"quote\"):\n",
    "        text = quote.find(\"span\", class_=\"text\").get_text()\n",
    "        author = quote.find(\"small\", class_=\"author\").get_text()\n",
    "        tags = [tag.get_text() for tag in quote.find_all(\"a\", class_=\"tag\")]\n",
    "        quotes_data.append({\n",
    "            \"text\": text,\n",
    "            \"author\": author,\n",
    "            \"tags\": tags\n",
    "        })\n",
    "\n",
    "    next_btn = soup.find(\"li\", class_=\"next\")\n",
    "    if next_btn:\n",
    "        next_link = next_btn.find(\"a\")[\"href\"]\n",
    "        url = \"https://quotes.toscrape.com\" + next_link\n",
    "    else:\n",
    "        url = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(quotes_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Salvataggio dei dati in CSV\n",
    "\n",
    "Salviamo la lista di citazioni estratte in un file CSV, per poterla usare o analizzare successivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_filename = \"quotes.csv\"\n",
    "with open(csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"text\", \"author\", \"tags\"])\n",
    "    writer.writeheader()\n",
    "    for row in quotes_data:\n",
    "        row[\"tags\"] = \", \".join(row[\"tags\"])\n",
    "        writer.writerow(row)\n",
    "print(f\"Dati salvati su {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Esempio di autenticazione Basic HTTP\n",
    "\n",
    "Se un sito è protetto da autenticazione Basic HTTP, puoi passare le credenziali con `requests`.\n",
    "\n",
    "Per esempio, se un sito richiede username e password:\n",
    "\n",
    "- user: `admin`\n",
    "- password: `password123`\n",
    "\n",
    "Puoi fare così:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_url = \"https://httpbin.org/basic-auth/admin/password123\"\n",
    "response_auth = requests.get(auth_url, auth=(\"admin\", \"password123\"))\n",
    "\n",
    "if response_auth.status_code == 200:\n",
    "    print(\"Autenticazione riuscita\")\n",
    "    data = response_auth.json()\n",
    "    print(data)\n",
    "else:\n",
    "    print(f\"Autenticazione fallita con codice {response_auth.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## 6. Analisi del file robots.txt\n",
    "\n",
    "Il file `robots.txt` è uno standard usato dai siti web per indicare ai bot (crawler, scraper) quali pagine possono o non possono essere visitate.\n",
    "\n",
    "### Perché è importante leggere `robots.txt`?\n",
    "- Rispettare le regole del sito web (etica del scraping)\n",
    "- Evitare di essere bloccati o bannati\n",
    "- Comprendere quali sezioni sono accessibili ai bot\n",
    "\n",
    "---\n",
    "\n",
    "Un esempio di file `robots.txt` può essere:\n",
    "\n",
    "User-agent: *\n",
    "Disallow: /private/\n",
    "Allow: /public/\n",
    "\n",
    "Significa che tutti i bot (`*`) non possono visitare `/private/`, ma possono visitare `/public/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_robots_txt(base_url):\n",
    "    if not base_url.endswith('/'):\n",
    "        base_url += '/'\n",
    "    robots_url = base_url + \"robots.txt\"\n",
    "    response = requests.get(robots_url)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "site = \"https://www.wikipedia.org\"\n",
    "robots_txt = fetch_robots_txt(site)\n",
    "print(\"robots.txt content:\\n\")\n",
    "print(robots_txt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Interpretazione semplificata del file robots.txt\n",
    "\n",
    "Vediamo ora come estrarre le regole principali:\n",
    "\n",
    "- **User-agent**: definisce a quali bot si applicano le regole successive\n",
    "- **Disallow**: percorsi vietati ai bot specificati\n",
    "- **Allow**: percorsi esplicitamente permessi\n",
    "\n",
    "Qui mostreremo un parser semplice che stampa le regole per tutti i bot (`*`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_robots_txt(robots_txt):\n",
    "    lines = robots_txt.splitlines()\n",
    "    user_agent = None\n",
    "    rules = {\"Disallow\": [], \"Allow\": []}\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith(\"#\"):\n",
    "            continue\n",
    "        if line.lower().startswith(\"user-agent:\"):\n",
    "            user_agent = line.split(\":\", 1)[1].strip()\n",
    "        elif line.lower().startswith(\"disallow:\") and user_agent == \"*\":\n",
    "            path = line.split(\":\", 1)[1].strip()\n",
    "            if path:\n",
    "                rules[\"Disallow\"].append(path)\n",
    "        elif line.lower().startswith(\"allow:\") and user_agent == \"*\":\n",
    "            path = line.split(\":\", 1)[1].strip()\n",
    "            if path:\n",
    "                rules[\"Allow\"].append(path)\n",
    "    return rules\n",
    "\n",
    "if robots_txt:\n",
    "    rules = parse_robots_txt(robots_txt)\n",
    "    print(\"Regole per User-agent '*':\")\n",
    "    print(\"Disallow:\", rules[\"Disallow\"])\n",
    "    print(\"Allow:\", rules[\"Allow\"])\n",
    "else:\n",
    "    print(\"Nessun file robots.txt trovato o non accessibile.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Conclusioni\n",
    "\n",
    "In questo notebook abbiamo visto come:\n",
    "\n",
    "- Fare scraping semplice con `requests` e `BeautifulSoup`\n",
    "- Estrarre dati e salvarli in CSV\n",
    "- Gestire autenticazione Basic HTTP\n",
    "- Scaricare e interpretare il file `robots.txt` per rispettare le regole di scraping\n",
    "\n",
    "Leggere e rispettare il file `robots.txt` è fondamentale per uno scraping etico e sostenibile.\n",
    "\n",
    "Per pagine con contenuti caricati dinamicamente via JavaScript, si può ricorrere a strumenti più avanzati come Selenium o Playwright, che permettono di eseguire un browser reale o headless e interagire con la pagina.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
