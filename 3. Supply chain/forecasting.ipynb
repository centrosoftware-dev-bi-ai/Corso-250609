{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Previsione delle vendite con variabili esogene\n",
    "\n",
    "## Contesto\n",
    "\n",
    "L'obiettivo di questo notebook è mostrare, in modo semplice e didattico, come si possa prevedere la domanda giornaliera di un prodotto utilizzando anche **variabili esterne** (dette \"esogene\"), come il prezzo, le promozioni o il meteo.\n",
    "\n",
    "Simuleremo dei dati e useremo tre modelli di previsione, confrontandone i risultati. I passaggi sono gli stessi che si seguirebbero in un progetto reale di previsione della domanda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Simulazione del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_days = 365 * 2\n",
    "date_range = pd.date_range(start=datetime.datetime.now(), periods=n_days, freq=\"D\")\n",
    "\n",
    "temperature = (\n",
    "    10\n",
    "    + 15 * np.sin(2 * np.pi * date_range.dayofyear / 365)\n",
    "    + np.random.normal(0, 2, n_days)\n",
    ")\n",
    "price = np.random.uniform(8, 12, n_days)\n",
    "promotion = np.random.binomial(1, 0.3, n_days)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"temperature\": temperature,\n",
    "        \"price\": price,\n",
    "        \"promotion\": promotion,\n",
    "    },\n",
    "    index=date_range,\n",
    ")\n",
    "\n",
    "target = (\n",
    "    50\n",
    "    - 2 * price\n",
    "    + 20 * promotion\n",
    "    + 0.5 * temperature\n",
    "    - 0.05 * temperature**2\n",
    "    + 10 * np.sin(2 * np.pi * date_range.dayofyear / 365)\n",
    "    + 5 * np.cos(2 * np.pi * date_range.dayofyear / (365 / 2))\n",
    "    + np.random.normal(0, 3, n_days)\n",
    ")\n",
    "\n",
    "df[\"sales\"] = pd.Series(target, index=date_range, name=\"sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Che cosa contiene il nostro dataset?\n",
    "\n",
    "Ogni riga del dataset rappresenta un giorno. Per ogni giorno abbiamo:\n",
    "\n",
    "- `sales`: il numero di vendite giornaliere (da prevedere)\n",
    "- `price`: il prezzo del prodotto in quel giorno\n",
    "- `promotion`: una variabile che vale 1 se il prodotto è in promozione, 0 altrimenti\n",
    "- `temperature`: la temperatura del giorno (es. può influire sulle vendite stagionali)\n",
    "\n",
    "Abbiamo anche introdotto un po' di **rumore casuale**, per simulare l'imprevedibilità reale del mercato."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Analisi esplorativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"sales\", \"price\", \"promotion\", \"temperature\"]].plot(subplots=True, figsize=(12, 8))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Perché osserviamo i dati?\n",
    "\n",
    "Guardare i dati nel tempo ci permette di:\n",
    "\n",
    "- Capire se ci sono stagionalità settimanali o annuali\n",
    "- Vedere l’effetto di promozioni e temperature\n",
    "- Identificare anomalie o trend\n",
    "\n",
    "Questa fase visiva è fondamentale per guidare le decisioni successive sui modelli da usare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Analisi della Stagionalità\n",
    "\n",
    "La stagionalità rappresenta pattern ricorrenti e prevedibili nei dati nel tempo, come variazioni mensili o trimestrali nelle vendite dovute a fattori stagionali (es. festività, stagioni climatiche, abitudini di consumo).\n",
    "\n",
    "Indagare la stagionalità ci permette di migliorare la qualità delle previsioni, introducendo variabili che aiutano i modelli a catturare questi effetti ricorrenti.\n",
    "\n",
    "In questa sezione:\n",
    "- Visualizzeremo la stagionalità tramite decomposizione della serie.\n",
    "- Creeremo variabili temporali per includere la stagionalità nei modelli di previsione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = seasonal_decompose(df[\"sales\"], model=\"additive\", period=12)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(411)\n",
    "plt.plot(df[\"sales\"], label=\"Originale\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.subplot(412)\n",
    "plt.plot(result.trend, label=\"Trend\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.subplot(413)\n",
    "plt.plot(result.seasonal, label=\"Stagionalità\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.subplot(414)\n",
    "plt.plot(result.resid, label=\"Residui\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Aggiungiamo variabili stagionali, medie mobili e lag per catturare la dinamica temporale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"dayofweek\"] = df.index.dayofweek\n",
    "df[\"month\"] = df.index.month\n",
    "df = pd.get_dummies(df, columns=[\"dayofweek\", \"month\"], drop_first=True)\n",
    "\n",
    "df[\"sales_ma7\"] = df[\"sales\"].rolling(window=7).mean()\n",
    "df[\"sales_lag1\"] = df[\"sales\"].shift(1)\n",
    "\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Cosa sono le \"feature\" e perché le costruiamo?\n",
    "\n",
    "Le **feature** (o variabili esplicative) sono gli elementi che useremo per prevedere le vendite. Oltre a quelle già presenti nel dataset (prezzo, promozione...), aggiungiamo:\n",
    "\n",
    "- Giorno della settimana (`dayofweek`) e mese (`month`) per catturare la stagionalità\n",
    "- Media mobile a 7 giorni (`sales_ma7`): serve a catturare la tendenza recente\n",
    "- Valore del giorno precedente (`sales_lag1`): perché le vendite di oggi sono spesso simili a quelle di ieri\n",
    "\n",
    "Queste trasformazioni aiutano i modelli a cogliere meglio i comportamenti ricorrenti."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Divisione tra dati di addestramento e di test\n",
    "\n",
    "In ogni progetto di previsione è fondamentale **valutare le prestazioni del modello su dati \"mai visti\" prima**, cioè non usati per costruirlo. Per questo, dividiamo i dati in due parti:\n",
    "\n",
    "- **Training set**: i primi 80% dei giorni. Servono per far \"imparare\" il modello.\n",
    "- **Test set**: gli ultimi 20% dei giorni. Servono per verificare se il modello sa generalizzare su dati nuovi.\n",
    "\n",
    "Poiché stiamo lavorando con dati **temporali**, è molto importante **rispettare l’ordine cronologico**: non possiamo mescolare i giorni casualmente, altrimenti finiremmo per \"barare\", usando informazioni dal futuro per prevedere il passato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcoliamo l'indice di separazione: 80% per il training, 20% per il test\n",
    "split_index = int(len(df) * 0.8)\n",
    "\n",
    "# Usiamo solo le variabili rilevanti per la previsione\n",
    "features = [\"price\", \"promotion\", \"temperature\"]\n",
    "target = \"sales\"\n",
    "\n",
    "# Dividiamo i dati mantenendo l'ordine temporale\n",
    "X_train = df[features].iloc[:split_index]\n",
    "X_test = df[features].iloc[split_index:]\n",
    "\n",
    "y_train = df[target].iloc[:split_index]\n",
    "y_test = df[target].iloc[split_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Perché normalizziamo i dati?\n",
    "\n",
    "Molti modelli di machine learning (come la regressione lineare e le reti neurali) funzionano meglio se tutte le variabili numeriche sono **sullo stesso ordine di grandezza**.\n",
    "\n",
    "Ad esempio, il prezzo potrebbe variare tra 5 e 10, mentre le vendite vanno da 50 a 200. Questo può sbilanciare il modello.\n",
    "\n",
    "Per questo, applichiamo una **normalizzazione** (scaling) che trasforma ogni variabile in modo che abbia:\n",
    "\n",
    "- media = 0  \n",
    "- deviazione standard = 1\n",
    "\n",
    "È importante eseguire questo processo **solo sui dati di addestramento**, e poi applicarlo ai dati di test, per non introdurre informazioni future nella fase di training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creiamo uno scaler e lo \"addestriamo\" solo sui dati di training\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Applichiamo la stessa trasformazione ai dati di test\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Cosa sono i modelli di regressione?\n",
    "\n",
    "Un **modello di regressione** cerca di trovare una relazione matematica tra le feature (es. prezzo, promozione...) e il valore che vogliamo prevedere (vendite).\n",
    "\n",
    "Cominceremo con un modello molto semplice: la **Ridge Regression**, che è una variante della regressione lineare. È utile come punto di partenza perché è veloce, interpretabile e spesso sorprendentemente efficace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge()\n",
    "param_grid = {\"alpha\": [0.1, 1.0, 10.0, 100.0]}\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "grid = GridSearchCV(ridge, param_grid, cv=tscv)\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_ridge = grid.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Cos’è XGBoost?\n",
    "\n",
    "**XGBoost** è un modello più sofisticato. Si basa su una tecnica chiamata *alberi decisionali a gradient boosting*. Funziona bene anche in situazioni non lineari e con molte interazioni tra le variabili.\n",
    "\n",
    "Rispetto alla regressione lineare, può cogliere relazioni più complesse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBRegressor(n_estimators=100, learning_rate=0.1)\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Cosa sono le reti neurali?\n",
    "\n",
    "Le **reti neurali** sono modelli ispirati al cervello umano. Sono molto flessibili e capaci di adattarsi a una vasta gamma di problemi. Qui useremo una rete semplice (2 layer nascosti) per vedere come si comporta rispetto ai modelli precedenti.\n",
    "\n",
    "Tuttavia, sono meno interpretabili e richiedono un po' più di dati per dare il meglio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        Input(shape=(X_train_scaled.shape[1],)),\n",
    "        Dense(32, activation=\"relu\"),\n",
    "        Dense(16, activation=\"relu\"),\n",
    "        Dense(1),\n",
    "    ]\n",
    ")\n",
    "model.compile(optimizer=Adam(learning_rate=0.01), loss=\"mse\")\n",
    "\n",
    "es = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "model.fit(\n",
    "    X_train_scaled, y_train, validation_split=0.2, epochs=100, callbacks=[es], verbose=0\n",
    ")\n",
    "\n",
    "y_pred_mlp = model.predict(X_test_scaled).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Valutazione dei modelli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred, name):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = root_mean_squared_error(y_true, y_pred)\n",
    "    print(f\"{name} → MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "\n",
    "\n",
    "evaluate(y_test, y_pred_ridge, \"Ridge\")\n",
    "evaluate(y_test, y_pred_xgb, \"XGBoost\")\n",
    "evaluate(y_test, y_pred_mlp, \"MLP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Come si valuta un modello?\n",
    "\n",
    "Per confrontare i modelli usiamo due indicatori:\n",
    "\n",
    "- **MAE**: Errore Assoluto Medio, misura quanto in media sbaglia il modello\n",
    "- **RMSE**: Scarto Quadratico Medio, penalizza maggiormente gli errori grandi\n",
    "\n",
    "Più questi valori sono bassi, migliore è la previsione."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Confronto delle predizioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test.index, y_test, label=\"True\")\n",
    "plt.plot(y_test.index, y_pred_ridge, label=\"Ridge\")\n",
    "plt.plot(y_test.index, y_pred_xgb, label=\"XGBoost\")\n",
    "plt.plot(y_test.index, y_pred_mlp, label=\"MLP\")\n",
    "plt.legend()\n",
    "plt.title(\"Confronto delle predizioni\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### Cosa osserviamo nel confronto?\n",
    "\n",
    "Guardando il grafico, possiamo confrontare:\n",
    "\n",
    "- L’andamento reale delle vendite (**True**)\n",
    "- Le previsioni dei diversi modelli\n",
    "\n",
    "Questo ci aiuta a capire visivamente quale modello segue meglio la realtà e se ci sono differenze significative tra le soluzioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "ape_ridge = np.abs((y_test - y_pred_ridge) / y_test) * 100\n",
    "ape_xgb = np.abs((y_test - y_pred_xgb) / y_test) * 100\n",
    "ape_mlp = np.abs((y_test - y_pred_mlp) / y_test) * 100\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5), sharey=True)\n",
    "\n",
    "axes[0].hist(ape_ridge, bins=30, color=\"tab:blue\", alpha=0.7)\n",
    "axes[0].set_title(\"Ridge\")\n",
    "axes[0].set_xlabel(\"Errore Percentuale Assoluto (%)\")\n",
    "axes[0].set_ylabel(\"Frequenza\")\n",
    "\n",
    "axes[1].hist(ape_xgb, bins=30, color=\"tab:orange\", alpha=0.7)\n",
    "axes[1].set_title(\"XGBoost\")\n",
    "axes[1].set_xlabel(\"Errore Percentuale Assoluto (%)\")\n",
    "\n",
    "axes[2].hist(ape_mlp, bins=30, color=\"tab:green\", alpha=0.7)\n",
    "axes[2].set_title(\"MLP\")\n",
    "axes[2].set_xlabel(\"Errore Percentuale Assoluto (%)\")\n",
    "\n",
    "plt.suptitle(\"Distribuzione degli errori percentuali assoluti per modello\")\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### Distribuzione degli errori percentuali assoluti per modello\n",
    "\n",
    "Il grafico mostra tre istogrammi affiancati, uno per ciascun modello di previsione (Ridge, XGBoost e MLP). Ogni istogramma rappresenta la distribuzione dell'errore percentuale assoluto (APE) calcolato per ogni osservazione nel test set.\n",
    "\n",
    "L’errore percentuale assoluto misura la deviazione relativa tra il valore predetto e quello reale, espressa in percentuale. Ad esempio, un valore del 5% indica che la previsione si discosta dal dato reale del 5%.\n",
    "\n",
    "Questi istogrammi permettono di confrontare in modo visivo:\n",
    "- La precisione complessiva di ogni modello (quanto sono concentrati gli errori vicino allo zero).\n",
    "- La presenza di errori molto grandi (code dell’istogramma verso valori alti).\n",
    "- La consistenza della previsione nel tempo.\n",
    "\n",
    "In sintesi, modelli con una distribuzione degli errori più stretta e concentrata verso valori bassi sono da preferire, perché forniscono previsioni più accurate e stabili."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## Conclusioni\n",
    "\n",
    "In questo notebook abbiamo implementato e confrontato tre diversi modelli per la previsione delle vendite con variabili esogene: Ridge Regression, XGBoost e una rete neurale MLP.\n",
    "\n",
    "Dal confronto emergono alcune evidenze importanti:\n",
    "\n",
    "- **Importanza della normalizzazione e dello split temporale:** Il corretto preprocessing dei dati è essenziale per evitare informazioni spurie e garantire una stima realistica delle performance future.\n",
    "\n",
    "- **Analisi della stagionalità:** L’inserimento di variabili che rappresentano la stagionalità migliora la capacità predittiva dei modelli, evidenziando come la componente temporale ciclica sia fondamentale nella previsione delle vendite.\n",
    "\n",
    "- **Analisi degli errori:** L’istogramma degli errori percentuali assoluti permette di valutare non solo l’accuratezza media, ma anche la distribuzione e la frequenza degli errori maggiori, fornendo una visione più completa della qualità delle previsioni.\n",
    "\n",
    "In sintesi, la combinazione di tecniche di machine learning con un’accurata fase di esplorazione e preprocessing dei dati rappresenta la strategia migliore per affrontare problemi di forecasting complessi come quello presentato. Per applicazioni reali, è consigliabile testare ulteriori modelli e iperparametri, nonché monitorare costantemente le performance con dati aggiornati.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
